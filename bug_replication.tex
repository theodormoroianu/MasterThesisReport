\chapter{Developping a DBMS Transactional Testing Framework}

\section{Overview}

This chapter presents the design, implementation and usage of a testing framework for replicating DBMS transactional bugs. Using the testing framework, we replicate a set of transactional bugs in the \textit{MySQL}, \textit{MariaDB} and \textit{TiDB} DBMSs. We then analyse the reports of the replicated bugs, and we explore the corelation between isolation levels and the reported bugs.

\section{Design}

The testing framework, is implemented in \textit{Python}, and heavily relies on \textit{Podman}, a container manager \cite{podmanwebpage} for managing DBMS instances. The tool works on \code{x64 GNU/Linux} systems, and we developped it in \textit{VSCode}, with the help of \textit{Github Copilot} \cite{copilotwebpage}.


\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{assets/replbug_design.png}
    \caption{Design of the \textit{ReplBug} testing framework}
    \label{fig:replb_design}
\end{figure}


The framework is modular, helping any future developer to easily extend it (for instance for adding support for new DBMSs). The main components in the bug testing pipeline (see Figure \ref{fig:replb_design}) are the following:

\begin{itemize}
    \item The \textit{podman connector}: This component handles the interaction with the \textit{Podman} engine, and is responsible for starting, stopping, downloading and managing containers running DBMS instances.
    \item The \textit{test parser}: This component handles the parsing of testcases, using a specific format, and is responsible for creating the internal representation of the testcases.
    \item The \textit{mysql connector}: This component handles the connection to a DBMS instance (running within a container), and is responsible for executing statements in order and extracting the results.
    \item The \textit{transaction runner}: This component handles the execution of all the statements in a transaction, and runs on different threads for concurrency.
    \item The \textit{scenario runner}: This component runs testcases under a specific configuration.
    \item The \textit{test runner}: This component orchestrates the execution of all required testcases under all specified configurations. 
\end{itemize}


\section{Usage}



\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{assets/replbug_shell.png}
    \caption{Using \textit{ReplBug} to start 2 \textit{MySQL v8.0.30} shells}
    \label{fig:replb_shell}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{assets/replbug_server.png}
    \caption{Using \textit{ReplBug} to start a \textit{TiDB v6.5.11} server}
    \label{fig:repl_server}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{assets/replbug_test.png}
    \caption{Using \textit{ReplBug} to generate reports of some known bugs}
    \label{fig:repl_test}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{assets/replbug_interactive.png}
    \caption{Using \textit{ReplBug} in interactive mode}
    \label{fig:repl_interactive}
\end{figure}




The testing framework, called \textit{ReplBug} is invoked from the CLI. The main features it offers, exposed by the executable as subcommands are the following:
\begin{itemize}
    \item \textbf{\code{shell}} (See Figure \ref{fig:replb_shell}): Starts one or multiple \textit{MySQL}, \textit{MariaDB} or \textit{TiDB} shells, connected to a specific version of the DBMS. If the version is not present on the local machine, the tool will attempt to pull the image from Docker Hub.
    \item \textbf{\code{server}} (See Figure \ref{fig:repl_server}): Starts a specific version of the \textit{MySQL}, \textit{MariaDB} or \textit{TiDB} DBMS and provides the required details (host, port, user) for connecting to the server.
    \item \textbf{\code{test}} (See Figure \ref{fig:repl_test}): Runs the scenarios of some known bugs (which have to be written in a specific format prior), and automatically generates reports of the execution.
    \item \textbf{\code{list}}: Returns a list of the testcases available in the tool (optionally a \textit{regex} can be passed to filter the results).
\end{itemize}

The tool can be either used from the CLI by passing arguments, or in interactive mode, where the tool exposes a shell that can be used by the user  (see Figure \ref{fig:repl_interactive}).

% % WHAT IS BELOW NOT WRITEN BY ME
% Guided by \textbf{NOC-NOC}, \cite{multazzu2023nocs} designed two algorithms for two concurrency control levels: Read Atomicy and Transactional Causal Consistency. Interested readers can refer to \cite{multazzu2023nocs} for more details. Essentially, the client maintains version vectors to view the database states.  It also holds two views of the database states: the global safe vector version and the local safe vector version. By maintaining these metadata, the client could check the incoming transactions version and its local versions to maintain safety properties within one round of communication, thus satisfying the \textbf{O} property: \textit{One Round of Communication}. However, this is not without its penalty. It has to recursively scan all the dual states in the database to find the most recent version before being committed.  In contrast, the Eiger \cite{lloyd2013stronger} and the Eiger-Port \cite{lu2020performance} algorithms may incur additional network round trips, but require less local computation overhead. 


% Although there are many technical details behind these algorithms, there is one \textit{fundamental} thinking.  To successfully commit a distributed transaction, we have to pay some cost. This cost could be paid either via network communication or via local computation on the client side or on the server side. The actual transaction performance depends on the performance of the hardware (local computation CPU, network bandwidth) and the distribution of the cost each hardware has to pay. It is a common belief that in the distributed transaction protocol, we should minimize the network round trip as much as possible. However, this is usually at the cost of increasing local computation. If we increase the local computation burden and the local CPU becomes over-utilized, but the network becomes under-utilized, this may lead to suboptimal performance.

% \textbf{NOC-NOC} principles and algorithms guided by these principles seek to minimize the network round trip and increase local computation overhead compared with algorithms such as Eiger \cite{lloyd2013stronger} and Eiger-PORT \cite{lu2020performance}. Although \cite{multazzu2023nocs} has shown that \textbf{NOC-NOC} is generally considered better than these two algorithms, in this chapter, we try to test these algorithms under certain corner cases and try to understand the local computation overhead between these algorithms and design principles.
% \section{Experimental Setup}
% \paragraph{Code Base.} For Eiger \cite{lloyd2013stronger} and Eiger-PORT \cite{lu2020performance}, we used its original code base. For \textbf{RA-NOC2} and \textbf{Eiger-NOC2}, we also used its original code base. 

% \paragraph{Workload.} We used the YCSB benchmark \cite{cooper2010benchmarking}. YCSB is a key-value benchmark that Yahoo collects for many usages, including testing the performance of key-value stores, different concurrency control protocols. We mainly used workload A for testing. Workload A is a read-dominant workload, but the read-write ratio could be tuned for different purposes. Unless otherwise stated, we use the default parameter settings in YCSB.

% \paragraph{Setup.} We used the CloudLab \cite{duplyakin2019design} as the testbed.  We used the XL170 node on the Utah cluster. The machine has 2 10-Core, 3.4 GHz, Xeon E5-2640 v4 CPUs and the network bandwidth is 10Gbps. 

% \paragraph{Measurements.} 
% In each of the experiments, we measure the overall throughput, average latency, and the 99th latency. We run experiments for two different concurrency levels: Read Atomic and Causal Consistency. Unless otherwise stated, we used the default parameters. By default, each transaction contains 8 operations. The default zipfian factor is 0 (uniform distribution). We used 5 servers and 5 clients by default. The value size is 64B and the number of keys is 1K by default. We run the experiment 5 times and take the average.

% \section{Experimental Results}
% \subsection{Large Transaction Size}
% We experiment with different concurrency control algorithms concerning large transaction sizes. The first three figures are at the Read Atomic level. The last three are at the causal consistent level.
%  Here are the results. Refer to Figure \ref{fig:1}, Figure \ref{fig:2}, Figure \ref{fig:3}, Figure \ref{fig:4}, Figure \ref{fig:5}, Figure \ref{fig:6}. Results show that NOC2 based algorithms outperform all other baselines under large transaction size.
% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/1-ra-t.png}
%     \caption{Throughput Comparison for Three Different Concurrency Control Algorithms under Different Transaction Size in Read Atomicity Level}
%     \label{fig:1}
% \end{figure}



% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/1-ra-al.png}
%     \caption{Average Latency Comparison for Three Different Concurrency Control Algorithms under Different Transaction Size in Read Atomicity Level}
%     \label{fig:2}
% \end{figure}


% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/1-ra-l.png}
%     \caption{99th Latency Comparison for Three Different Concurrency Control Algorithms under Different Transaction Size in Read Atomicity Level}
%     \label{fig:3}
% \end{figure}


% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/1-tcc-t.png}
%     \caption{Throughput Comparison for Three Different Concurrency Control Algorithms under Different Transaction Size in Causal Consistency Level}
%     \label{fig:4}
% \end{figure}


% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/1-tcc-al.png}
%     \caption{Average Latency Comparison for Three Different Concurrency Control Algorithms under Different Transaction Size in Causal Consistency Level}
%     \label{fig:5}
% \end{figure}


% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/1-tcc-l.png}
%     \caption{99th Latency Comparison for Three Different Concurrency Control Algorithms under Different Transaction Size in Causal Consistency Level}
%     \label{fig:6}
% \end{figure}

% \newpage

% \subsection{Large Number of Servers}
% We experiment with different concurrency control algorithms concerning large number of servers. The first three figures are at the Read Atomic level. The last three are at the causal consistent level. Here are the results. Refer to Figure ~\ref{fig:7}, Figure ~\ref{fig:8}, Figure ~\ref{fig:9}, Figure \ref{fig:10}, Figure \ref{fig:11}, Figure \ref{fig:12}.  Results show that NOC2 based algorithms outperform all other baselines under different number of servers.


% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/3-ra-t.png}
%     \caption{Throughput Comparison for Three Different Concurrency Control Algorithms under Different Number of Servers in Read Atomicity Level}
%     \label{fig:7}
% \end{figure}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/3-ra-al.png}
%     \caption{Average Latency Comparison for Three Different Concurrency Control Algorithms under Different Number of Servers in Read Atomicity Level}
%     \label{fig:8}
% \end{figure}


% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/3-ra-l.png}
%     \caption{99th Latency Comparison for Three Different Concurrency Control Algorithms under Different Number of Servers in Read Atomicity Level}
%     \label{fig:9}
% \end{figure}


% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/3-tcc-t.png}
%     \caption{Throughput Comparison for Three Different Concurrency Control Algorithms under Different Number of Servers in Causal Consistency Level}
%     \label{fig:10}
% \end{figure}


% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/3-tcc-al.png}
%     \caption{Average Latency Comparison for Three Different Concurrency Control Algorithms under Different Number of Servers in Causal Consistency Level}
%     \label{fig:11}
% \end{figure}


% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/3-tcc-l.png}
%     \caption{99th Latency Comparison for Three Different Concurrency Control Algorithms under Different Number of Servers in Causal Consistency Level}
%     \label{fig:12}
% \end{figure}


% \newpage


% \subsection{Large Number of Clients}
% We experiment with different concurrency control algorithms concerning large number of clients. The first three figures are at the Read Atomic level. The last three are at the causal consistent level. Here are the results. Refer to Figure ~\ref{fig:13}, Figure ~\ref{fig:14}, Figure ~\ref{fig:15}, Figure \ref{fig:16}, Figure \ref{fig:17}, Figure \ref{fig:18}. Results show that NOC2 based algorithms outperform all other baselines under different number of clients. 



% \begin{figure}[H]
%     \centering
% \includegraphics[width=0.8\linewidth]{figure/5-ra-t.png}
%     \caption{Throughput Comparison for Three Different Concurrency Control Algorithms under Different Number of Clients in Read Atomicity Level}
%     \label{fig:13}
% \end{figure}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/5-ra-al.png}
%     \caption{Average Latency Comparison for Three Different Concurrency Control Algorithms under Different Number of Clients in Read Atomicity Level}
%     \label{fig:14}
% \end{figure}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/5-ra-l.png}
%     \caption{99th Latency Comparison for Three Different Concurrency Control Algorithms under Different Number of Clients in Read Atomicity Level}
%     \label{fig:15}
% \end{figure}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/5-tcc-t.png}
%     \caption{Throughput Comparison for Three Different Concurrency Control Algorithms under Different Number of Clients in Causal Consistency Level}
%     \label{fig:16}
% \end{figure}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/5-tcc-al.png}
%     \caption{Average Latency Comparison for Three Different Concurrency Control Algorithms under Different Number of Clients in Causal Consistency Level}
%     \label{fig:17}
% \end{figure}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/5-tcc-l.png}
%     \caption{99th Latency Comparison for Three Different Concurrency Control Algorithms under Different Number of Clients in Causal Consistency Level}
%     \label{fig:18}
% \end{figure}

% \subsection{Large Value Size}

% We experiment with different concurrency control algorithms concerning large value size. The first three figures are at the Read Atomic level. The last three are at the causal consistent level. Here are the results. Refer to Figure ~\ref{fig:19}, Figure ~\ref{fig:20}, Figure ~\ref{fig:21}, Figure \ref{fig:22}, Figure \ref{fig:23}, Figure \ref{fig:24}. 
%  Results show that NOC2 based algorithms outperform all other baselines under different value size.
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/5-ra-t.png}
%     \caption{Throughput Comparison for Three Different Concurrency Control Algorithms under Different Value Size in Read Atomicity Level}
%     \label{fig:19}
% \end{figure}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/5-ra-al.png}
%     \caption{Average Latency Comparison for Three Different Concurrency Control Algorithms under Different Value Size in Read Atomicity Level}
%     \label{fig:20}
% \end{figure}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/5-ra-l.png}
%     \caption{99th Latency Comparison for Three Different Concurrency Control Algorithms under Different Value Size in Read Atomicity Level}
%     \label{fig:21}
% \end{figure}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/5-tcc-t.png}
%     \caption{Throughput Comparison for Three Different Value Size under Different Number of Clients in Causal Consistency Level}
%     \label{fig:22}
% \end{figure}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/5-tcc-al.png}
%     \caption{Average Latency Comparison for Three Different Concurrency Control Algorithms under Different Value Size in Causal Consistency Level}
%     \label{fig:23}
% \end{figure}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/5-tcc-l.png}
%     \caption{99th Latency Comparison for Three Different Concurrency Control Algorithms under Different Value Size in Causal Consistency Level}
%     \label{fig:24}
% \end{figure}



% \subsection{Large Number of Keys}

% We experiment with different concurrency control algorithms concerning large number of keys. The first three figures are at the Read Atomic level. The last three are at the causal consistent level. Here are the results. Refer to Figure ~\ref{fig:25}, Figure ~\ref{fig:26}, Figure ~\ref{fig:27}, Figure \ref{fig:28}, Figure \ref{fig:29}, Figure \ref{fig:30}. 

% Results show that NOC2 based algorithms outperform all other baselines under large number of keys.
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/6-ra-t.png}
%     \caption{Throughput Comparison for Three Different Concurrency Control Algorithms under Different Number of Keys in Read Atomicity Level}
%     \label{fig:25}
% \end{figure}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/6-ra-al.png}
%     \caption{Average Latency Comparison for Three Different Concurrency Control Algorithms under Different Number of Keys in Read Atomicity Level}
%     \label{fig:26}
% \end{figure}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/6-ra-l.png}
%     \caption{99th Latency Comparison for Three Different Concurrency Control Algorithms under Different Number of Keys in Read Atomicity Level}
%     \label{fig:27}
% \end{figure}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/6-tcc-t.png}
%     \caption{Throughput Comparison for Three Different Concurrency Control Algorithms under Different Number of Keys in Causal Consistency Level}
%     \label{fig:28}
% \end{figure}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/6-tcc-al.png}
%     \caption{Average Latency Comparison for Three Different Concurrency Control Algorithms under Different Number of Keys in Causal Consistency Level}
%     \label{fig:29}
% \end{figure}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/6-tcc-l.png}
%     \caption{99th Latency Comparison for Three Different Concurrency Control Algorithms under Different Number of Keys in Causal Consistency Level}
%     \label{fig:30}
% \end{figure}


% \subsection{Conclusions}
% In this chapter, we have conducted extensive experiments to compare algorithms guided by \textbf{NOC-NOC} principles vs. Eiger \cite{lloyd2013stronger}, Eiger-PORT \cite{lu2020performance} and RAMP family algorithms \cite{bailis2016scalable}. Experiment results show that \textbf{NOC-NOC} guided algorithms outperform all other algorithms in these settings. This means that in the current hardware landscape, moving costs from network communication to local computation is always beneficial.  In the future, when the network speed increases significantly, this conclusion may change.